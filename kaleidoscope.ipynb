{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t0K0UGfg0JpI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import ImageDraw\n",
        "import kornia.geometry\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Separate the three parts of the picture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part was written by our assistant, Martin Everaert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ST-PD_rELljq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def translate_each_sector_to_center_soft(image, center, angle, move_distance):\n",
        "    # Get image dimensions\n",
        "    batch_size, channels, height, width = image.size()\n",
        "    angles = angle + torch.Tensor([0.0, 2*np.pi/3, 4*np.pi/3])\n",
        "\n",
        "    # Convert center coordinates to image coordinates\n",
        "    center_x = center[0] * width\n",
        "    center_y = center[1] * height\n",
        "\n",
        "    # Create grid of coordinates\n",
        "    grid_y, grid_x = torch.meshgrid(torch.arange(height), torch.arange(width))\n",
        "    grid_x = grid_x.float().to(image.device)\n",
        "    grid_y = grid_y.float().to(image.device)\n",
        "\n",
        "    # Calculate angles of each pixel relative to the center\n",
        "    pixel_angles = torch.atan2(grid_y - center_y, grid_x - center_x)\n",
        "    pixel_angles = (pixel_angles + 2 * np.pi) % (2 * np.pi)\n",
        "\n",
        "\n",
        "    transformed_images = []\n",
        "    masks = []\n",
        "\n",
        "    for i in range(3):\n",
        "\n",
        "        # Compute the starting and ending angles for this sector\n",
        "        start_angle = angles[i]\n",
        "        end_angle = angles[i+1] if i<len(angles)-1 else angles[0]\n",
        "\n",
        "        start_direction = torch.tensor([torch.cos(start_angle+np.pi/2), torch.sin(start_angle+np.pi/2)], dtype=torch.float32)\n",
        "        end_direction = torch.tensor([torch.cos(end_angle+np.pi/2), torch.sin(end_angle+np.pi/2)], dtype=torch.float32)\n",
        "\n",
        "        # Compute signed distances for each pixel\n",
        "        point_vectors = torch.stack([grid_x - center_x, grid_y - center_y])\n",
        "        start_signed_distance = torch.sum(point_vectors * start_direction[:, None, None], dim=0)\n",
        "        end_signed_distance = torch.sum(point_vectors * end_direction[:, None, None], dim=0)\n",
        "\n",
        "        # Apply sigmoid to create soft mask\n",
        "        start_mask = torch.sigmoid(start_signed_distance/10-1)\n",
        "        end_mask = torch.sigmoid(-end_signed_distance/10-1)\n",
        "        mask = start_mask * end_mask\n",
        "\n",
        "\n",
        "        if start_angle <= end_angle:\n",
        "            angle = (start_angle + end_angle) / 2\n",
        "        else:\n",
        "            # Handling wrap-around when end_angle < start_angle, eg start = 2pi - 1 and end = 1\n",
        "            angle = ((start_angle + end_angle + 2 * torch.pi) / 2) % (2 * torch.pi)\n",
        "\n",
        "        # Compute the translation for this sector\n",
        "        move_distance_scaled = move_distance*width\n",
        "\n",
        "        M = kornia.geometry.get_affine_matrix2d(\n",
        "            translations = torch.stack([-move_distance_scaled * torch.cos(angle), -move_distance_scaled  * torch.sin(angle)], dim=1),\n",
        "            center = torch.stack([center_x, center_y]).unsqueeze(0),\n",
        "            scale = torch.ones(2).unsqueeze(0),\n",
        "            angle = torch.zeros(1)\n",
        "            )[:, :2, :]\n",
        "\n",
        "\n",
        "        # Apply the mask and translation\n",
        "        masked_image = image * mask.unsqueeze(0)\n",
        "        transformed_image = kornia.geometry.warp_affine(masked_image, M, dsize=(height, width, ))\n",
        "        transformed_mask = kornia.geometry.warp_affine(mask.unsqueeze(0).unsqueeze(0), M, dsize=(height, width, ))\n",
        "\n",
        "        transformed_images.append(transformed_image)\n",
        "        masks.append(transformed_mask)\n",
        "\n",
        "    return transformed_images[0], transformed_images[1], transformed_images[2], masks[0], masks[1], masks[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4WI5kJzMLp2E"
      },
      "outputs": [],
      "source": [
        "class KaleidoscopeModelSoft(nn.Module):\n",
        "    def __init__(self, center_init, angle_init, move_distance_init):\n",
        "        super(KaleidoscopeModelSoft, self).__init__()\n",
        "        # Define the parameters of the kaleidoscope effect\n",
        "        self.center = torch.tensor(center_init) #nn.Parameter(torch.tensor(center_init))\n",
        "        self.angle = nn.Parameter(torch.tensor([angle_init]))\n",
        "        self.move_distance =  nn.Parameter(torch.tensor([move_distance_init]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return translate_each_sector_to_center_soft(x, self.center, self.angle, self.move_distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f3H-C2IQNCFM",
        "outputId": "95283b77-cc51-4007-c083-1f9adc4ac4f2"
      },
      "outputs": [],
      "source": [
        "# Load the image using PIL\n",
        "image_path = \"pen.jpg\"\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Convert the image to a PyTorch tensor\n",
        "transform = transforms.ToTensor()\n",
        "image_tensor = transform(image)\n",
        "\n",
        "# Ensure the image tensor is in the correct shape and range\n",
        "image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
        "image_tensor = image_tensor.float()  # Convert to float32\n",
        "image_tensor /= 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "# Create the kaleidoscope model\n",
        "model = KaleidoscopeModelSoft(\n",
        "    center_init = [0.6, 0.6],\n",
        "    angle_init = 0.0,\n",
        "    move_distance_init = 0.2,\n",
        ")\n",
        "\n",
        "# Define the loss function\n",
        "criterion_l1 = nn.L1Loss()\n",
        "criterion_l2 = nn.MSELoss()\n",
        "alpha = 1e11\n",
        "beta = 1\n",
        "gamma = 1\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "vizualise = True\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    transformed_0, transformed_1, transformed_2, mask_0, mask_1, mask_2 = model(image_tensor)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss_img = alpha * criterion_l2(mask_0*mask_1*transformed_0, mask_0*mask_1*transformed_1)/torch.sum(mask_0*mask_1)\n",
        "    loss_img += alpha * criterion_l2(mask_0*mask_2*transformed_0, mask_0*mask_2*transformed_2)/torch.sum(mask_0*mask_2)\n",
        "    loss_img += alpha * criterion_l2(mask_1*mask_2*transformed_1, mask_1*mask_2*transformed_2)/torch.sum(mask_1*mask_2)\n",
        "\n",
        "    # Some regularization terms\n",
        "    loss_reg_center = beta * criterion_l2(model.center, 0.5*torch.ones(2))\n",
        "    loss_reg_move_distance = gamma * criterion_l2(model.move_distance, 0.2*torch.ones(1))\n",
        "\n",
        "    loss = loss_img + loss_reg_center + loss_reg_move_distance\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #print(model.angle.grad)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], loss_img: {loss_img.item()}, loss_reg_center: {loss_reg_center.item()}, loss_reg_move_distance: {loss_reg_move_distance.item()}')\n",
        "    print(model.center, model.angle, model.move_distance)\n",
        "\n",
        "\n",
        "    if vizualise and epoch == 199:\n",
        "        with torch.no_grad():\n",
        "            transformed_images_0_pil = transforms.ToPILImage()(transformed_0.squeeze(0)*255)\n",
        "            transformed_images_1_pil = transforms.ToPILImage()(transformed_1.squeeze(0)*255)\n",
        "            transformed_images_2_pil = transforms.ToPILImage()(transformed_2.squeeze(0)*255)\n",
        "            mask_0_pil = transforms.ToPILImage()(mask_0.squeeze(0))\n",
        "            mask_1_pil = transforms.ToPILImage()(mask_1.squeeze(0))\n",
        "            mask_2_pil = transforms.ToPILImage()(mask_2.squeeze(0))\n",
        "\n",
        "            transformed_images_total = torch.stack([transformed_0, transformed_1, transformed_2], dim=1).sum(dim=1)\n",
        "            transformed_images_total /= transformed_images_total.max()\n",
        "            transformed_images_total_pil = transforms.ToPILImage()(transformed_images_total.squeeze(0))\n",
        "\n",
        "            # Convert the image tensor to a PIL image\n",
        "            original_image_pil = transforms.ToPILImage()(image_tensor.squeeze(0)*255)\n",
        "\n",
        "            # Create a draw object\n",
        "            draw = ImageDraw.Draw(original_image_pil)\n",
        "\n",
        "            # Define colors for drawing\n",
        "            boundary_color = (0, 255, 0)  # Green color for boundaries\n",
        "\n",
        "\n",
        "            center_x, center_y = int(model.center[0] * original_image_pil.width), int(model.center[1] * original_image_pil.height)\n",
        "\n",
        "            # Draw boundaries\n",
        "            for angle_tmp in model.angle + torch.Tensor([0.0, 2*np.pi/3, 4*np.pi/3]):\n",
        "                x = center_x + model.move_distance.item() * np.cos(angle_tmp.item()) * original_image_pil.width\n",
        "                y = center_y + model.move_distance.item() * np.sin(angle_tmp.item()) * original_image_pil.width\n",
        "                draw.line([(center_x, center_y), (x, y)], fill=boundary_color, width=2)\n",
        "\n",
        "            display(original_image_pil) # Display the original image with center point and boundaries\n",
        "            display(transformed_images_total_pil)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the images\n",
        "transformed_images_0_pil.save(\"transformed_image_0.jpg\")\n",
        "transformed_images_1_pil.save(\"transformed_image_1.jpg\")\n",
        "transformed_images_2_pil.save(\"transformed_image_2.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Find matching points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "img2 = cv2.imread('transformed_image_2.jpg',cv2.IMREAD_GRAYSCALE) \n",
        "img1 = cv2.imread('transformed_image_1.jpg',cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Find keypoints with ORB from opencv\n",
        "orb = cv2.ORB_create()\n",
        "kp1, des1 = orb.detectAndCompute(img1,None)\n",
        "kp2, des2 = orb.detectAndCompute(img2,None)\n",
        " \n",
        "# Create matches with BFMatcher\n",
        "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "matches = bf.match(des1,des2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_horizontal_matches(matches, keypoints1, keypoints2, threshold=5):\n",
        "    good_matches = []\n",
        "    \n",
        "    for match in matches:\n",
        "        pt1 = np.array(keypoints1[match.queryIdx].pt)\n",
        "        pt2 = np.array(keypoints2[match.trainIdx].pt)\n",
        "        \n",
        "        # Check if the y-coordinates are within the threshold\n",
        "        if abs(pt1[1] - pt2[1]) < threshold:\n",
        "            length = np.linalg.norm(pt2 - pt1)\n",
        "            good_matches.append((match, length))\n",
        "            \n",
        "    return good_matches\n",
        "\n",
        "\n",
        "# Filter horizontal matches and calculate their lengths\n",
        "horizontal_matches_with_lengths = filter_horizontal_matches(matches, kp1, kp2)\n",
        "horizontal_matches = [match for match, length in horizontal_matches_with_lengths]\n",
        "lengths = [length for match, length in horizontal_matches_with_lengths]\n",
        "\n",
        "# Draw the matches\n",
        "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, horizontal_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "\n",
        "for match, length in horizontal_matches_with_lengths:\n",
        "    pt1 = np.array(kp1[match.queryIdx].pt)\n",
        "    pt2 = np.array(kp2[match.trainIdx].pt) + np.array([img1.shape[1], 0]) \n",
        "    mid_point = (pt1 + pt2) / 2\n",
        "    cv2.putText(img_matches, f'{length:.2f}', tuple(mid_point.astype(int)),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "# Display the images with epipolar lines\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(img_matches)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Plot keypoints with relative depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "# Add the original image as background\n",
        "fig.add_trace(go.Image(z=transformed_images_1_pil))\n",
        "\n",
        "for match, length in horizontal_matches_with_lengths:\n",
        "    pt1 = np.array(kp1[match.queryIdx].pt)\n",
        "    fig.add_trace(go.Scatter(x=[pt1[0]], y=[pt1[1]],\n",
        "                             mode='markers',\n",
        "                             marker=dict(color='red', size=5),\n",
        "                             hoverinfo='text',\n",
        "                             text=f'depth: {(length-1)*5:.2f}', # Transform lengths to depth\n",
        "                             showlegend=False))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title=\"Keypoints with Match Relative Depth\",\n",
        "    hovermode='closest',\n",
        "    showlegend=False,\n",
        "    height=img1.shape[0] * 3 //2 \n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
